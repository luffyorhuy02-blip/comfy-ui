{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# üé® ComfyUI FULL NODES + All Workflows\n",
                "\n",
                "**ƒê√£ c√†i s·∫µn:**\n",
                "- ‚úÖ 20+ Custom Nodes\n",
                "- ‚úÖ 11 Workflows s·∫µn s√†ng d√πng\n",
                "- ‚úÖ Models cho t·∫•t c·∫£ workflows\n",
                "- ‚úÖ Domain: comfyui.maymoc.shop"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#@title 1Ô∏è‚É£ üîÑ KEEP ALIVE\n",
                "import time, random, threading, gc\n",
                "from IPython.display import display, Javascript\n",
                "\n",
                "display(Javascript('''\n",
                "(function() {\n",
                "    let c = 0;\n",
                "    function run() {\n",
                "        c++;\n",
                "        document.querySelector(\"colab-connect-button\")?.click();\n",
                "        window.scrollBy(0, Math.random()*20-10);\n",
                "        var el = document.getElementById('ka');\n",
                "        if(el) el.innerHTML = 'üîÑ '+c;\n",
                "        setTimeout(run, 25000 + Math.random()*15000);\n",
                "    }\n",
                "    var d = document.createElement('div'); d.id='ka';\n",
                "    d.innerHTML = 'üîÑ ON';\n",
                "    d.style.cssText = 'position:fixed;top:10px;right:10px;background:linear-gradient(135deg,#667eea,#764ba2);color:white;padding:10px 20px;border-radius:25px;z-index:9999;';\n",
                "    document.body.appendChild(d);\n",
                "    run();\n",
                "})();\n",
                "'''))\n",
                "\n",
                "threading.Thread(target=lambda: [time.sleep(random.randint(30,90)) or gc.collect() for _ in iter(int,1)], daemon=True).start()\n",
                "print(\"‚úÖ Keep Alive ON\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#@title 2Ô∏è‚É£ Ki·ªÉm tra GPU\n",
                "!nvidia-smi\n",
                "import torch\n",
                "print(f\"\\n‚úÖ CUDA: {torch.cuda.is_available()} | GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'N/A'}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#@title 3Ô∏è‚É£ C√†i ComfyUI\n",
                "%cd /content\n",
                "!git clone https://github.com/comfyanonymous/ComfyUI.git\n",
                "%cd ComfyUI\n",
                "!pip install -q torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
                "!pip install -q -r requirements.txt\n",
                "!pip install -q xformers\n",
                "print(\"‚úÖ ComfyUI installed\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#@title 4Ô∏è‚É£ üì¶ C√ÄI FULL CUSTOM NODES\n",
                "%cd /content/ComfyUI/custom_nodes\n",
                "\n",
                "nodes = [\n",
                "    (\"https://github.com/ltdrdata/ComfyUI-Manager.git\", \"ComfyUI Manager\"),\n",
                "    (\"https://github.com/ltdrdata/ComfyUI-Impact-Pack.git\", \"Impact Pack\"),\n",
                "    (\"https://github.com/ltdrdata/ComfyUI-Inspire-Pack.git\", \"Inspire Pack\"),\n",
                "    (\"https://github.com/Fannovel16/comfyui_controlnet_aux.git\", \"ControlNet Aux\"),\n",
                "    (\"https://github.com/Kosinkadink/ComfyUI-Advanced-ControlNet.git\", \"Advanced ControlNet\"),\n",
                "    (\"https://github.com/cubiq/ComfyUI_IPAdapter_plus.git\", \"IPAdapter Plus\"),\n",
                "    (\"https://github.com/Gourieff/comfyui-reactor-node.git\", \"Reactor\"),\n",
                "    (\"https://github.com/Kosinkadink/ComfyUI-AnimateDiff-Evolved.git\", \"AnimateDiff\"),\n",
                "    (\"https://github.com/Kosinkadink/ComfyUI-VideoHelperSuite.git\", \"Video Helper\"),\n",
                "    (\"https://github.com/FizzleDorf/ComfyUI_FizzNodes.git\", \"Fizz Nodes\"),\n",
                "    (\"https://github.com/ssitu/ComfyUI_UltimateSDUpscale.git\", \"Ultimate Upscale\"),\n",
                "    (\"https://github.com/city96/ComfyUI-GGUF.git\", \"GGUF\"),\n",
                "    (\"https://github.com/jags111/efficiency-nodes-comfyui.git\", \"Efficiency\"),\n",
                "    (\"https://github.com/WASasquatch/was-node-suite-comfyui.git\", \"WAS Suite\"),\n",
                "    (\"https://github.com/pythongosssss/ComfyUI-Custom-Scripts.git\", \"Custom Scripts\"),\n",
                "    (\"https://github.com/rgthree/rgthree-comfy.git\", \"rgthree\"),\n",
                "    (\"https://github.com/Suzie1/ComfyUI_Comfyroll_CustomNodes.git\", \"Comfyroll\"),\n",
                "    (\"https://github.com/melMass/comfy_mtb.git\", \"MTB\"),\n",
                "    (\"https://github.com/Acly/comfyui-inpaint-nodes.git\", \"Inpaint\"),\n",
                "    (\"https://github.com/kijai/ComfyUI-KJNodes.git\", \"KJ Nodes\"),\n",
                "]\n",
                "\n",
                "for i, (url, name) in enumerate(nodes, 1):\n",
                "    !git clone -q {url} 2>/dev/null && echo \"‚úÖ [{i}/{len(nodes)}] {name}\" || echo \"‚è≠Ô∏è [{i}/{len(nodes)}] {name} (exists)\"\n",
                "\n",
                "print(f\"\\n‚úÖ Installed {len(nodes)} custom nodes!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#@title 5Ô∏è‚É£ üì• C√†i Dependencies\n",
                "%cd /content/ComfyUI/custom_nodes\n",
                "\n",
                "!pip install -q -r comfyui_controlnet_aux/requirements.txt 2>/dev/null\n",
                "!pip install -q -r comfyui-reactor-node/requirements.txt 2>/dev/null\n",
                "!pip install -q -r was-node-suite-comfyui/requirements.txt 2>/dev/null\n",
                "!pip install -q -r ComfyUI-VideoHelperSuite/requirements.txt 2>/dev/null\n",
                "!pip install -q -r ComfyUI-AnimateDiff-Evolved/requirements.txt 2>/dev/null\n",
                "!pip install -q -r ComfyUI_IPAdapter_plus/requirements.txt 2>/dev/null\n",
                "!pip install -q insightface onnxruntime-gpu opencv-python mediapipe imageio imageio-ffmpeg ffmpeg-python kornia ultralytics\n",
                "\n",
                "print(\"‚úÖ Dependencies installed\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#@title 6Ô∏è‚É£ üìã T·∫†O 11 WORKFLOWS S·∫¥N D√ôNG\n",
                "import os\n",
                "import json\n",
                "\n",
                "# T·∫°o th∆∞ m·ª•c workflows\n",
                "workflows_dir = \"/content/ComfyUI/user/default/workflows\"\n",
                "os.makedirs(workflows_dir, exist_ok=True)\n",
                "\n",
                "# ===== WORKFLOW 1: Basic Text to Image =====\n",
                "wf_basic = {\n",
                "  \"last_node_id\": 7,\n",
                "  \"last_link_id\": 6,\n",
                "  \"nodes\": [\n",
                "    {\"id\": 1, \"type\": \"CheckpointLoaderSimple\", \"pos\": [50, 100], \"widgets_values\": [\"sd_xl_base_1.0.safetensors\"], \"title\": \"üéØ Model\"},\n",
                "    {\"id\": 2, \"type\": \"CLIPTextEncode\", \"pos\": [400, 50], \"widgets_values\": [\"beautiful landscape, mountains, sunset, 8k, masterpiece\"], \"title\": \"‚úÖ Positive\"},\n",
                "    {\"id\": 3, \"type\": \"CLIPTextEncode\", \"pos\": [400, 200], \"widgets_values\": [\"ugly, blurry, low quality\"], \"title\": \"‚ùå Negative\"},\n",
                "    {\"id\": 4, \"type\": \"EmptyLatentImage\", \"pos\": [400, 350], \"widgets_values\": [1024, 1024, 1], \"title\": \"üìê Size\"},\n",
                "    {\"id\": 5, \"type\": \"KSampler\", \"pos\": [750, 100], \"widgets_values\": [0, \"randomize\", 25, 7.5, \"euler_ancestral\", \"normal\", 1], \"title\": \"üé≤ Sampler\"},\n",
                "    {\"id\": 6, \"type\": \"VAEDecode\", \"pos\": [1100, 100]},\n",
                "    {\"id\": 7, \"type\": \"SaveImage\", \"pos\": [1100, 200], \"widgets_values\": [\"ComfyUI\"], \"title\": \"üíæ Save\"}\n",
                "  ]\n",
                "}\n",
                "\n",
                "# ===== WORKFLOW 2: LoRA =====\n",
                "wf_lora = {\n",
                "  \"last_node_id\": 8,\n",
                "  \"nodes\": [\n",
                "    {\"id\": 1, \"type\": \"CheckpointLoaderSimple\", \"pos\": [50, 100], \"widgets_values\": [\"sd_xl_base_1.0.safetensors\"], \"title\": \"üéØ Model\"},\n",
                "    {\"id\": 2, \"type\": \"LoraLoader\", \"pos\": [50, 250], \"widgets_values\": [\"your_lora.safetensors\", 0.8, 0.8], \"title\": \"üîß LoRA\"},\n",
                "    {\"id\": 3, \"type\": \"CLIPTextEncode\", \"pos\": [400, 50], \"widgets_values\": [\"trigger_word, portrait, 8k\"], \"title\": \"‚úÖ Positive\"},\n",
                "    {\"id\": 4, \"type\": \"CLIPTextEncode\", \"pos\": [400, 200], \"widgets_values\": [\"ugly, blurry\"], \"title\": \"‚ùå Negative\"},\n",
                "    {\"id\": 5, \"type\": \"EmptyLatentImage\", \"pos\": [400, 350], \"widgets_values\": [1024, 1024, 1]},\n",
                "    {\"id\": 6, \"type\": \"KSampler\", \"pos\": [750, 100], \"widgets_values\": [0, \"randomize\", 30, 7, \"dpmpp_2m\", \"karras\", 1]},\n",
                "    {\"id\": 7, \"type\": \"VAEDecode\", \"pos\": [1100, 100]},\n",
                "    {\"id\": 8, \"type\": \"SaveImage\", \"pos\": [1100, 200], \"widgets_values\": [\"LoRA\"]}\n",
                "  ]\n",
                "}\n",
                "\n",
                "# ===== WORKFLOW 3: Img2Img =====\n",
                "wf_img2img = {\n",
                "  \"nodes\": [\n",
                "    {\"id\": 1, \"type\": \"CheckpointLoaderSimple\", \"widgets_values\": [\"sd_xl_base_1.0.safetensors\"]},\n",
                "    {\"id\": 2, \"type\": \"LoadImage\", \"title\": \"üì∑ Input Image\"},\n",
                "    {\"id\": 3, \"type\": \"VAEEncode\"},\n",
                "    {\"id\": 4, \"type\": \"CLIPTextEncode\", \"widgets_values\": [\"anime style, vibrant\"], \"title\": \"‚úÖ Style\"},\n",
                "    {\"id\": 5, \"type\": \"CLIPTextEncode\", \"widgets_values\": [\"ugly, blurry\"]},\n",
                "    {\"id\": 6, \"type\": \"KSampler\", \"widgets_values\": [0, \"randomize\", 25, 7, \"euler_ancestral\", \"normal\", 0.6], \"title\": \"denoise 0.6\"},\n",
                "    {\"id\": 7, \"type\": \"VAEDecode\"},\n",
                "    {\"id\": 8, \"type\": \"SaveImage\", \"widgets_values\": [\"Img2Img\"]}\n",
                "  ]\n",
                "}\n",
                "\n",
                "# ===== WORKFLOW 4: Hyper-FLUX 8 Steps =====\n",
                "wf_flux = {\n",
                "  \"nodes\": [\n",
                "    {\"id\": 1, \"type\": \"UNETLoader\", \"widgets_values\": [\"flux1-dev.safetensors\", \"fp8_e4m3fn\"], \"title\": \"üéØ FLUX Model\"},\n",
                "    {\"id\": 2, \"type\": \"LoraLoader\", \"widgets_values\": [\"Hyper-FLUX.1-dev-8steps-lora.safetensors\", 0.125, 1], \"title\": \"‚ö° Hyper-FLUX LoRA\"},\n",
                "    {\"id\": 3, \"type\": \"DualCLIPLoader\", \"widgets_values\": [\"t5xxl_fp8_e4m3fn.safetensors\", \"clip_l.safetensors\", \"flux\"]},\n",
                "    {\"id\": 4, \"type\": \"VAELoader\", \"widgets_values\": [\"ae.safetensors\"]},\n",
                "    {\"id\": 5, \"type\": \"CLIPTextEncode\", \"widgets_values\": [\"beautiful portrait, 8k\"]},\n",
                "    {\"id\": 6, \"type\": \"EmptySD3LatentImage\", \"widgets_values\": [1024, 1024, 1]},\n",
                "    {\"id\": 7, \"type\": \"KSampler\", \"widgets_values\": [0, \"randomize\", 8, 1.0, \"euler\", \"simple\", 1], \"title\": \"‚ö° 8 Steps Only!\"},\n",
                "    {\"id\": 8, \"type\": \"VAEDecode\"},\n",
                "    {\"id\": 9, \"type\": \"SaveImage\", \"widgets_values\": [\"HyperFLUX\"]}\n",
                "  ]\n",
                "}\n",
                "\n",
                "# ===== WORKFLOW 5: ControlNet =====\n",
                "wf_controlnet = {\n",
                "  \"nodes\": [\n",
                "    {\"id\": 1, \"type\": \"CheckpointLoaderSimple\", \"widgets_values\": [\"sd_xl_base_1.0.safetensors\"]},\n",
                "    {\"id\": 2, \"type\": \"LoadImage\", \"title\": \"üì∑ Pose/Edge Image\"},\n",
                "    {\"id\": 3, \"type\": \"ControlNetLoader\", \"widgets_values\": [\"control_v11p_sd15_openpose.pth\"]},\n",
                "    {\"id\": 4, \"type\": \"AIO_Preprocessor\", \"widgets_values\": [\"CannyEdgePreprocessor\", 512], \"title\": \"üîç Preprocessor\"},\n",
                "    {\"id\": 5, \"type\": \"ControlNetApplyAdvanced\", \"widgets_values\": [1.0, 0, 1]},\n",
                "    {\"id\": 6, \"type\": \"CLIPTextEncode\", \"widgets_values\": [\"beautiful woman, studio lighting\"]},\n",
                "    {\"id\": 7, \"type\": \"CLIPTextEncode\", \"widgets_values\": [\"ugly, blurry\"]},\n",
                "    {\"id\": 8, \"type\": \"KSampler\"},\n",
                "    {\"id\": 9, \"type\": \"VAEDecode\"},\n",
                "    {\"id\": 10, \"type\": \"SaveImage\", \"widgets_values\": [\"ControlNet\"]}\n",
                "  ]\n",
                "}\n",
                "\n",
                "# ===== WORKFLOW 6: Face Swap =====\n",
                "wf_faceswap = {\n",
                "  \"nodes\": [\n",
                "    {\"id\": 1, \"type\": \"LoadImage\", \"title\": \"üë§ Source Face\"},\n",
                "    {\"id\": 2, \"type\": \"LoadImage\", \"title\": \"üéØ Target Image\"},\n",
                "    {\"id\": 3, \"type\": \"ReActorFaceSwap\", \"widgets_values\": [True, \"inswapper_128.onnx\", \"retinaface_resnet50\", \"GFPGANv1.4.pth\", 1, 1, \"no\", \"no\", \"no\", 0, 1, \"0\", \"0\", 0.5], \"title\": \"üîÑ Face Swap\"},\n",
                "    {\"id\": 4, \"type\": \"SaveImage\", \"widgets_values\": [\"FaceSwap\"]}\n",
                "  ]\n",
                "}\n",
                "\n",
                "# ===== WORKFLOW 7: Upscale =====\n",
                "wf_upscale = {\n",
                "  \"nodes\": [\n",
                "    {\"id\": 1, \"type\": \"LoadImage\", \"title\": \"üì∑ Image to Upscale\"},\n",
                "    {\"id\": 2, \"type\": \"UpscaleModelLoader\", \"widgets_values\": [\"4x-UltraSharp.pth\"], \"title\": \"üîç Upscale Model\"},\n",
                "    {\"id\": 3, \"type\": \"ImageUpscaleWithModel\", \"title\": \"‚¨ÜÔ∏è Upscale 4x\"},\n",
                "    {\"id\": 4, \"type\": \"ImageScaleBy\", \"widgets_values\": [\"lanczos\", 0.5], \"title\": \"üìè Resize to 2x\"},\n",
                "    {\"id\": 5, \"type\": \"PreviewImage\", \"title\": \"üëÅÔ∏è Preview\"},\n",
                "    {\"id\": 6, \"type\": \"SaveImage\", \"widgets_values\": [\"Upscaled\"]}\n",
                "  ]\n",
                "}\n",
                "\n",
                "# ===== WORKFLOW 8: AnimateDiff Video =====\n",
                "wf_animatediff = {\n",
                "  \"nodes\": [\n",
                "    {\"id\": 1, \"type\": \"CheckpointLoaderSimple\", \"widgets_values\": [\"v1-5-pruned-emaonly.safetensors\"], \"title\": \"üéØ SD 1.5\"},\n",
                "    {\"id\": 2, \"type\": \"ADE_LoadAnimateDiffModel\", \"widgets_values\": [\"v3_sd15_mm.ckpt\"], \"title\": \"üé¨ Motion Model\"},\n",
                "    {\"id\": 3, \"type\": \"ADE_ApplyAnimateDiffModelSimple\"},\n",
                "    {\"id\": 4, \"type\": \"CLIPTextEncode\", \"widgets_values\": [\"woman dancing, flowing dress, dynamic\"]},\n",
                "    {\"id\": 5, \"type\": \"CLIPTextEncode\", \"widgets_values\": [\"ugly, static\"]},\n",
                "    {\"id\": 6, \"type\": \"EmptyLatentImage\", \"widgets_values\": [512, 512, 16], \"title\": \"16 frames\"},\n",
                "    {\"id\": 7, \"type\": \"KSampler\", \"widgets_values\": [0, \"randomize\", 20, 7.5, \"euler_ancestral\", \"normal\", 1]},\n",
                "    {\"id\": 8, \"type\": \"VAEDecode\"},\n",
                "    {\"id\": 9, \"type\": \"VHS_VideoCombine\", \"widgets_values\": [\"AnimateDiff\", \"video/h264-mp4\", False, 8, \"default\", \"output\", True], \"title\": \"üé¨ Save MP4\"}\n",
                "  ]\n",
                "}\n",
                "\n",
                "# ===== WORKFLOW 9: Video2Video =====\n",
                "wf_vid2vid = {\n",
                "  \"nodes\": [\n",
                "    {\"id\": 1, \"type\": \"VHS_LoadVideo\", \"widgets_values\": [\"input.mp4\", \"Uploaded\", 0, False, 24, 0, 64, \"Disabled\"], \"title\": \"üìπ Load Video\"},\n",
                "    {\"id\": 2, \"type\": \"CheckpointLoaderSimple\", \"widgets_values\": [\"v1-5-pruned-emaonly.safetensors\"]},\n",
                "    {\"id\": 3, \"type\": \"ADE_LoadAnimateDiffModel\", \"widgets_values\": [\"v3_sd15_mm.ckpt\"]},\n",
                "    {\"id\": 4, \"type\": \"ADE_ApplyAnimateDiffModelSimple\"},\n",
                "    {\"id\": 5, \"type\": \"VAEEncode\"},\n",
                "    {\"id\": 6, \"type\": \"CLIPTextEncode\", \"widgets_values\": [\"anime style, vibrant\"]},\n",
                "    {\"id\": 7, \"type\": \"CLIPTextEncode\", \"widgets_values\": [\"ugly\"]},\n",
                "    {\"id\": 8, \"type\": \"KSampler\", \"widgets_values\": [0, \"randomize\", 20, 7, \"euler_ancestral\", \"normal\", 0.5], \"title\": \"denoise 0.5\"},\n",
                "    {\"id\": 9, \"type\": \"VAEDecode\"},\n",
                "    {\"id\": 10, \"type\": \"VHS_VideoCombine\", \"widgets_values\": [\"Video2Video\", \"video/h264-mp4\", False, 24, \"default\", \"output\", True]}\n",
                "  ]\n",
                "}\n",
                "\n",
                "# ===== WORKFLOW 10: Outfit Swap =====\n",
                "wf_outfit = {\n",
                "  \"nodes\": [\n",
                "    {\"id\": 1, \"type\": \"CheckpointLoaderSimple\", \"widgets_values\": [\"sd_xl_base_1.0.safetensors\"]},\n",
                "    {\"id\": 2, \"type\": \"LoadImage\", \"title\": \"üë§ Person Photo\"},\n",
                "    {\"id\": 3, \"type\": \"LoadImage\", \"title\": \"üëî Garment Image\"},\n",
                "    {\"id\": 4, \"type\": \"IPAdapterModelLoader\", \"widgets_values\": [\"ip-adapter-plus_sdxl_vit-h.safetensors\"]},\n",
                "    {\"id\": 5, \"type\": \"CLIPVisionLoader\", \"widgets_values\": [\"CLIP-ViT-H-14-laion2B-s32B-b79K.safetensors\"]},\n",
                "    {\"id\": 6, \"type\": \"IPAdapterAdvanced\", \"widgets_values\": [1.0, \"style transfer\", \"concat\", 0, 1, \"V only\"], \"title\": \"üëî Apply Garment\"},\n",
                "    {\"id\": 7, \"type\": \"CLIPTextEncode\", \"widgets_values\": [\"fashion model, elegant dress\"]},\n",
                "    {\"id\": 8, \"type\": \"CLIPTextEncode\", \"widgets_values\": [\"ugly, naked\"]},\n",
                "    {\"id\": 9, \"type\": \"VAEEncode\"},\n",
                "    {\"id\": 10, \"type\": \"KSampler\", \"widgets_values\": [0, \"randomize\", 25, 7, \"dpmpp_2m\", \"karras\", 0.65], \"title\": \"denoise 0.65\"},\n",
                "    {\"id\": 11, \"type\": \"VAEDecode\"},\n",
                "    {\"id\": 12, \"type\": \"SaveImage\", \"widgets_values\": [\"OutfitSwap\"]}\n",
                "  ]\n",
                "}\n",
                "\n",
                "# ===== WORKFLOW 11: Multi-ControlNet Video =====\n",
                "wf_multicontrol = {\n",
                "  \"nodes\": [\n",
                "    {\"id\": 1, \"type\": \"CheckpointLoaderSimple\", \"widgets_values\": [\"v1-5-pruned-emaonly.safetensors\"]},\n",
                "    {\"id\": 2, \"type\": \"ADE_LoadAnimateDiffModel\", \"widgets_values\": [\"v3_sd15_mm.ckpt\"]},\n",
                "    {\"id\": 3, \"type\": \"ADE_ApplyAnimateDiffModelSimple\"},\n",
                "    {\"id\": 4, \"type\": \"VHS_LoadVideo\", \"title\": \"üìπ Reference Video\"},\n",
                "    {\"id\": 5, \"type\": \"ControlNetLoader\", \"widgets_values\": [\"control_v11p_sd15_openpose.pth\"], \"title\": \"üéÆ OpenPose\"},\n",
                "    {\"id\": 6, \"type\": \"ControlNetLoader\", \"widgets_values\": [\"control_v11f1p_sd15_depth.pth\"], \"title\": \"üéÆ Depth\"},\n",
                "    {\"id\": 7, \"type\": \"AIO_Preprocessor\", \"widgets_values\": [\"DWPreprocessor\", 512], \"title\": \"ü¶¥ Pose\"},\n",
                "    {\"id\": 8, \"type\": \"AIO_Preprocessor\", \"widgets_values\": [\"DepthAnythingPreprocessor\", 512], \"title\": \"üìè Depth\"},\n",
                "    {\"id\": 9, \"type\": \"ControlNetApplyAdvanced\", \"widgets_values\": [0.8, 0, 1], \"title\": \"Apply Pose 0.8\"},\n",
                "    {\"id\": 10, \"type\": \"ControlNetApplyAdvanced\", \"widgets_values\": [0.5, 0, 1], \"title\": \"Apply Depth 0.5\"},\n",
                "    {\"id\": 11, \"type\": \"CLIPTextEncode\", \"widgets_values\": [\"anime girl dancing, vibrant\"]},\n",
                "    {\"id\": 12, \"type\": \"CLIPTextEncode\", \"widgets_values\": [\"ugly, static\"]},\n",
                "    {\"id\": 13, \"type\": \"EmptyLatentImage\", \"widgets_values\": [512, 512, 32], \"title\": \"32 frames\"},\n",
                "    {\"id\": 14, \"type\": \"KSampler\"},\n",
                "    {\"id\": 15, \"type\": \"VAEDecode\"},\n",
                "    {\"id\": 16, \"type\": \"VHS_VideoCombine\", \"widgets_values\": [\"MultiControl\", \"video/h264-mp4\", False, 12, \"default\", \"output\", True]}\n",
                "  ]\n",
                "}\n",
                "\n",
                "# Save all workflows\n",
                "workflows = {\n",
                "    \"01_basic_txt2img.json\": wf_basic,\n",
                "    \"02_lora_txt2img.json\": wf_lora,\n",
                "    \"03_img2img.json\": wf_img2img,\n",
                "    \"04_hyper_flux_8steps.json\": wf_flux,\n",
                "    \"05_controlnet.json\": wf_controlnet,\n",
                "    \"06_faceswap.json\": wf_faceswap,\n",
                "    \"07_upscale.json\": wf_upscale,\n",
                "    \"08_animatediff_video.json\": wf_animatediff,\n",
                "    \"09_video2video.json\": wf_vid2vid,\n",
                "    \"10_outfit_swap.json\": wf_outfit,\n",
                "    \"11_multicontrol_video.json\": wf_multicontrol,\n",
                "}\n",
                "\n",
                "for name, wf in workflows.items():\n",
                "    with open(f\"{workflows_dir}/{name}\", \"w\") as f:\n",
                "        json.dump(wf, f, indent=2)\n",
                "    print(f\"‚úÖ {name}\")\n",
                "\n",
                "print(f\"\\nüìÅ Workflows saved to: {workflows_dir}\")\n",
                "print(\"üí° Trong ComfyUI: Load ‚Üí Browse ‚Üí ch·ªçn workflow\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#@title 7Ô∏è‚É£ Mount Google Drive\n",
                "from google.colab import drive\n",
                "import os\n",
                "\n",
                "drive.mount('/content/drive')\n",
                "\n",
                "folders = ['checkpoints', 'loras', 'vae', 'controlnet', 'ipadapter', 'clip_vision', 'upscale_models', 'animatediff_models', 'insightface', 'facerestore_models', 'outputs']\n",
                "for f in folders:\n",
                "    os.makedirs(f'/content/drive/MyDrive/ComfyUI/models/{f}', exist_ok=True)\n",
                "\n",
                "for folder in ['checkpoints', 'loras', 'vae', 'controlnet', 'clip_vision', 'upscale_models']:\n",
                "    !rm -rf /content/ComfyUI/models/{folder}\n",
                "    !ln -s /content/drive/MyDrive/ComfyUI/models/{folder} /content/ComfyUI/models/{folder}\n",
                "\n",
                "!rm -rf /content/ComfyUI/output\n",
                "!ln -s /content/drive/MyDrive/ComfyUI/models/outputs /content/ComfyUI/output\n",
                "\n",
                "!mkdir -p /content/ComfyUI/models/animatediff_models /content/ComfyUI/models/ipadapter /content/ComfyUI/models/insightface /content/ComfyUI/models/facerestore_models\n",
                "\n",
                "print(\"‚úÖ Drive linked\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#@title 8Ô∏è‚É£ üì• Download Models\n",
                "download_all = True  #@param {type:\"boolean\"}\n",
                "\n",
                "if download_all:\n",
                "    # Checkpoints\n",
                "    %cd /content/ComfyUI/models/checkpoints\n",
                "    !wget -q --show-progress -nc -O sd_xl_base_1.0.safetensors \"https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/resolve/main/sd_xl_base_1.0.safetensors\"\n",
                "    !wget -q --show-progress -nc -O v1-5-pruned-emaonly.safetensors \"https://huggingface.co/runwayml/stable-diffusion-v1-5/resolve/main/v1-5-pruned-emaonly.safetensors\"\n",
                "    \n",
                "    # ControlNet\n",
                "    %cd /content/ComfyUI/models/controlnet\n",
                "    !wget -q --show-progress -nc -O control_v11p_sd15_openpose.pth \"https://huggingface.co/lllyasviel/ControlNet-v1-1/resolve/main/control_v11p_sd15_openpose.pth\"\n",
                "    !wget -q --show-progress -nc -O control_v11f1p_sd15_depth.pth \"https://huggingface.co/lllyasviel/ControlNet-v1-1/resolve/main/control_v11f1p_sd15_depth.pth\"\n",
                "    !wget -q --show-progress -nc -O control_v11p_sd15_canny.pth \"https://huggingface.co/lllyasviel/ControlNet-v1-1/resolve/main/control_v11p_sd15_canny.pth\"\n",
                "    \n",
                "    # IPAdapter\n",
                "    %cd /content/ComfyUI/models/ipadapter\n",
                "    !wget -q --show-progress -nc -O ip-adapter-plus_sdxl_vit-h.safetensors \"https://huggingface.co/h94/IP-Adapter/resolve/main/sdxl_models/ip-adapter-plus_sdxl_vit-h.safetensors\"\n",
                "    %cd /content/ComfyUI/models/clip_vision\n",
                "    !wget -q --show-progress -nc -O CLIP-ViT-H-14-laion2B-s32B-b79K.safetensors \"https://huggingface.co/h94/IP-Adapter/resolve/main/models/image_encoder/model.safetensors\"\n",
                "    \n",
                "    # AnimateDiff\n",
                "    %cd /content/ComfyUI/models/animatediff_models\n",
                "    !wget -q --show-progress -nc -O v3_sd15_mm.ckpt \"https://huggingface.co/guoyww/animatediff/resolve/main/v3_sd15_mm.ckpt\"\n",
                "    \n",
                "    # Upscale\n",
                "    %cd /content/ComfyUI/models/upscale_models\n",
                "    !wget -q --show-progress -nc -O 4x-UltraSharp.pth \"https://huggingface.co/Kim2091/UltraSharp/resolve/main/4x-UltraSharp.pth\"\n",
                "    \n",
                "    # Face Swap\n",
                "    %cd /content/ComfyUI/models/insightface\n",
                "    !wget -q --show-progress -nc -O inswapper_128.onnx \"https://huggingface.co/deepinsight/inswapper/resolve/main/inswapper_128.onnx\"\n",
                "    %cd /content/ComfyUI/models/facerestore_models\n",
                "    !wget -q --show-progress -nc -O GFPGANv1.4.pth \"https://github.com/TencentARC/GFPGAN/releases/download/v1.3.4/GFPGANv1.4.pth\"\n",
                "\n",
                "print(\"\\n‚úÖ All models downloaded!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#@title 9Ô∏è‚É£ C√†i Cloudflare\n",
                "!wget -q https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64.deb && dpkg -i cloudflared-linux-amd64.deb\n",
                "print(\"‚úÖ Cloudflared installed\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#@title üîü üöÄ CH·∫†Y COMFYUI\n",
                "import subprocess, threading, time\n",
                "\n",
                "TOKEN = \"eyJhIjoiZWYyMWQ3NTM2YTgwNGQ4ZjY1OTZlY2VmZjU3NzVkOTciLCJ0IjoiMjM2MTU3ZWQtYjc3NC00YmJlLTk5ZjctYWFjZDJiMmRmMWQyIiwicyI6Ik16TTROVEkwTldJdE5qSXlNQzAwWldNeUxUZ3lZemt0TXpNek5EZG1abUprWmpVeiJ9\"\n",
                "DOMAIN = \"comfyui.maymoc.shop\"\n",
                "\n",
                "threading.Thread(target=lambda: subprocess.run([\"cloudflared\", \"tunnel\", \"run\", \"--token\", TOKEN]), daemon=True).start()\n",
                "time.sleep(5)\n",
                "\n",
                "print(\"\\n\" + \"=\"*60)\n",
                "print(f\"üåê https://{DOMAIN}\")\n",
                "print(\"=\"*60)\n",
                "print(\"‚úÖ 20+ Custom Nodes\")\n",
                "print(\"‚úÖ 11 Workflows (Load ‚Üí Browse)\")\n",
                "print(\"‚úÖ All Models Ready\")\n",
                "print(\"=\"*60 + \"\\n\")\n",
                "\n",
                "%cd /content/ComfyUI\n",
                "!python main.py --listen 0.0.0.0 --port 8188"
            ]
        }
    ],
    "metadata": {
        "accelerator": "GPU",
        "colab": {
            "gpuType": "T4"
        },
        "kernelspec": {
            "display_name": "Python 3",
            "name": "python3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 0
}